<!DOCTYPE html>
<html lang="en">

<head>
   <title>Data Leakage - Fall 2023</title>
   <link rel="stylesheet" href="../../../css/main.css">
</head>

   <body>
   <div class="topnav">
      <div class="heading">
         <h1>Data Leakage - Fall 2023</h1>
      </div>
      <div class="menu">
         <a href="../../about-us/">About the team</a>
         <a class="active" href="../../../index.html">About the project</a>
         <a href="../../design/">Design</a>
         <a href="../../tasks/">Tasks</a>
         <a href="../../resources/">Resources</a>
      </div>
   </div>
      <main>
         <h2>Preprocessing Leakage</h2>

         <h3>What is preprocessing leakage?</h3>
         <p>
            Preprocessing leakage occurs when information from the test set
            influences the preprocessing steps applied to the training set.
         </p>

         <h3>Causes of preprocessing leakage:</h3>
         <p>
            If preprocessing steps are influenced by information from the test
            set, it can lead to overly optimistic performance estimates during
            model development and may result in poor generalization to new,
            unseen data.
         </p>

         <h3>Solutions for preprocessing leakage:</h3>
         <ul>
            <li>
               Perform Preprocessing Independently: Apply preprocessing steps
               based on information from the train set without considering the
               test set.
            </li>

            <li>
               Use Pipelines: Implement preprocessing steps within a pipeline to
               ensure consistency and prevent information flow between the
               training and test sets.
            </li>

            <li>
               Handle Missing Values Appropriately: If missing values are
               imputed, use methods based solely on information from the
               training set. Avoid using global statistics or values derived
               from the test set.
            </li>
         </ul>

         <h3>Example of Preprocessing Leakage Code Displayed Below</h3>
         <p>
            Introducing preprocessing leakage means the tokenizer is mistakenly fitted on the entire dataset, including the validation and test sets. This will cause the tokenizer to learn from information that should be unseen during model development, leading to preprocessing leakage.
         </p>
         <pre>
            <code>
               import tensorflow as tf
               import pandas as pd
               import numpy as np
               import string
               import re
               import nltk
               from nltk.corpus import stopwords
               from sklearn.model_selection import train_test_split
               from tensorflow.keras.preprocessing.text import Tokenizer
               from tensorflow.keras.preprocessing.sequence import pad_sequences
               
               nltk.download('punkt')
               nltk.download('stopwords')
               
               # Read dataset
               dataset = pd.read_csv("/Users/Karina/Desktop/amazon_reviews.csv")
               
               # Lowercase reviewText column
               dataset['reviewText'] = dataset['reviewText'].str.lower()
               
               # Define preprocessing functions
               def no_punctuation_stopwords_numbers(text):
                   if isinstance(text, str):
                       text = re.sub(f"[{re.escape(string.punctuation)}0-9]", "", text)
                       tokens = nltk.word_tokenize(text)
                       new_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]
                       return ' '.join(new_tokens)
                   else:
                       return ""
               
               # Apply preprocessing functions
               dataset['new_reviewText'] = dataset['reviewText'].apply(no_punctuation_stopwords_numbers)
               
               # Fit tokenizer on the entire dataset (introducing preprocessing leakage)
               max_words = 10000
               max_sequence_length = 100
               tokenizer = Tokenizer(num_words=max_words)
               tokenizer.fit_on_texts(dataset['new_reviewText'])
               
               # Split data into train, validation, and test sets
               X_train, X_temp, y_train, y_temp = train_test_split(dataset['new_reviewText'], dataset['overall'], test_size=0.2, random_state=42)
               X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
               
               # Tokenize and pad sequences
               sequences_train = tokenizer.texts_to_sequences(X_train)
               sequences_valid = tokenizer.texts_to_sequences(X_valid)
               sequences_test = tokenizer.texts_to_sequences(X_test)
               
               X_train_padded = pad_sequences(sequences_train, maxlen=max_sequence_length, padding='post', truncating='post')
               X_valid_padded = pad_sequences(sequences_valid, maxlen=max_sequence_length, padding='post', truncating='post')
               X_test_padded = pad_sequences(sequences_test, maxlen=max_sequence_length, padding='post', truncating='post')
               
               # Print shapes of train, validation, and test sets
               print(X_train_padded.shape, y_train.shape)
               print(X_valid_padded.shape, y_valid.shape)
               print(X_test_padded.shape, y_test.shape)
            </code>
         </pre>

         <h3>Fix Preprocessing Leakage Displayed Below</h3>
         <p>
            Fix preprocessing leakage by ensuring that preprocessing steps are performed independently for the training, validation, and test sets.
            <ul>
               <li>The preprocessing function preprocess_text checks if the input is a string before applying preprocessing steps to avoid TypeError.</li>
               <li>Preprocessing is applied independently to each set after splitting the dataset to ensure that each set is processed separately, preventing preprocessing leakage.</li>
               <li>The tokenizer is fitted only on the training data, and then tokenization and padding are performed independently for each set using the fitted tokenizer, ensuring preprocessing consistency and preventing preprocessing leakage.</li>
            </ul>
         </p>

         <pre>
            <code>
               import tensorflow as tf
               import pandas as pd
               import numpy as np
               import string
               import re
               import nltk
               from nltk.corpus import stopwords
               from sklearn.model_selection import train_test_split
               from tensorflow.keras.preprocessing.text import Tokenizer
               from tensorflow.keras.preprocessing.sequence import pad_sequences
               
               nltk.download('punkt')
               nltk.download('stopwords')
               
               # Read dataset
               dataset = pd.read_csv("/Users/Karina/Desktop/amazon_reviews.csv")
               
               # Lowercase reviewText column
               dataset['reviewText'] = dataset['reviewText'].str.lower()
               
               # Define preprocessing functions
               def preprocess_text(text):
                   if isinstance(text, str):
                       text = re.sub(f"[{re.escape(string.punctuation)}0-9]", "", text)
                       tokens = nltk.word_tokenize(text)
                       new_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]
                       return ' '.join(new_tokens)
                   else:
                       return ""
               
               # Apply preprocessing independently to each set
               X_train, X_temp, y_train, y_temp = train_test_split(dataset['reviewText'], dataset['overall'], test_size=0.2, random_state=42)
               X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
               
               X_train = X_train.apply(preprocess_text)
               X_valid = X_valid.apply(preprocess_text)
               X_test = X_test.apply(preprocess_text)
               
               # Define tokenizer
               max_words = 10000
               max_sequence_length = 100
               tokenizer = Tokenizer(num_words=max_words)
               
               # Fit tokenizer on training data
               tokenizer.fit_on_texts(X_train)
               
               # Tokenize and pad sequences for each set
               X_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_sequence_length, padding='post', truncating='post')
               X_valid_padded = pad_sequences(tokenizer.texts_to_sequences(X_valid), maxlen=max_sequence_length, padding='post', truncating='post')
               X_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_sequence_length, padding='post', truncating='post')
               
               # Print shapes of train, validation, and test sets
               print(X_train_padded.shape, y_train.shape)
               print(X_valid_padded.shape, y_valid.shape)
               print(X_test_padded.shape, y_test.shape)
            </code>
         </pre>

         <div class="nav">
            <h4>Other types of leakage:</h4>
            <ul>
               <li>
                  <a href="../multi-test/">Multi-Test Leakage</a>
               </li>
               <li>
                  <a href="../overlap/">Overlap Leakage</a>
               </li>
            </ul>
         </div>
      </main>

      <script src="" async defer></script>
   </body>
</html>
