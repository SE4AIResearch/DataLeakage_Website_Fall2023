<!DOCTYPE html>
<html>
   <head>
      <title>Data Leakage - Fall 2023</title>
   </head>

   <body>
      <h1>Data Leakage - Fall 2023</h1>
      <h3>Project Description</h3>
      <p>
         Data science pipelines to train and evaluate models with machine
         learning may contain bugs just like any other code. Leakage between
         training and testing data can lead to overestimating the model's
         accuracy during offline evaluation's, possibly leading to deployment of
         low-quality models in production. Such leakage can be tedious to detect
         manually. Therefore, we develop a statistical approach to detect common
         forms of data leakage in data science. Detecting data leakage in
         machine learning models and developing an extension for it.
      </p>
      <h3>Team Members</h3>
      <p>
         Karina Berberian, Catherine DeMario, Brandon Kreiser, Suneedhi Laddha,
         Roger Shagawat, Cindy Tran
      </p>

      <ul>
         <li><a href="">Week 1</a></li>
      </ul>

      <h3>Time Tracking</h3>
      <div>
         <iframe
            src="https://docs.google.com/spreadsheets/d/e/2PACX-1vTFt2VHjBEGR_33HlGB845Aye0olyFW_0fbzena9cGb9uj5oYaTlZzIQFNu_CMgHXXfLUlWa4u4gVnV/pubhtml?widget=true&amp;headers=false"
            width=""
            height="250"
         ></iframe>
      </div>
   </body>
</html>
